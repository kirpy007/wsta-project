{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pprint\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing and tokenizing function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return [word for word in word_tokenize(stem_text(strip_punctuation(text.lower().replace(\"\\n\", \"\")))) if word not in ['lrb', 'rrb']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Initializing global variables </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "files = glob.glob('/home/rohan/Documents/Homework/WSTA/wiki-pages-text/*.txt')\n",
    "files = [file for file in files if file != '/home/rohan/Documents/Homework/WSTA/wiki-pages-text/to_test.txt']\n",
    "files = ['/home/rohan/Documents/Homework/WSTA/wiki-pages-text/wiki-001.txt']\n",
    "doc_dict = {}\n",
    "file_dict = {}\n",
    "df = pd.DataFrame()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "punc_table = str.maketrans({key: None for key in string.punctuation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Reading files into a dictionary </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rohan/Documents/Homework/WSTA/wiki-pages-text/wiki-001.txt\n",
      "Time taken:  0.20307421684265137\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for file in files:\n",
    "    with open(file, encoding = 'utf-8') as f:\n",
    "        print(file)\n",
    "        for line in f:\n",
    "            try:\n",
    "                #print(line)\n",
    "                line = line.split(maxsplit=2)\n",
    "                #print(line)\n",
    "                key = line[0]\n",
    "                value = line[2]\n",
    "                if key in file_dict.keys():\n",
    "                    file_dict[key] = file_dict[key] + value\n",
    "                else:\n",
    "                    file_dict[key] = value\n",
    "                    #print(value)\n",
    "            except Exception as e:\n",
    "                print(\"ERROR\",e, line)\n",
    "        for key, values in file_dict.items():\n",
    "            doc_dict[key] = values\n",
    "        #print(file_dict)\n",
    "print(\"Time taken: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Loading into Dataframe </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_dict.items(),columns=['doc_id','doc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pickle dataframe </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_name = file.split('/')[-1].split(\".\")[0]\n",
    "df.to_pickle(\"all_files\" + \".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load pickled dataframe </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('all_files.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print dataframe and preprocess document</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 48228\n",
      "Time taken: 20.75375771522522\n"
     ]
    }
   ],
   "source": [
    "#print(df.head())\n",
    "#print(df.head()['value'].tolist())\n",
    "start = time.time()\n",
    "df['doc_processed'] = df['doc'].apply(lambda x: preprocess(x))\n",
    "print(\"Number of documents:\", len(df))\n",
    "print(\"Time taken:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pickling preprocessed docs </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'punc_less_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-de557be72597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../preprocessed_docs.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc_less_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'punc_less_docs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create Gensim dictionary </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(60085 unique tokens: ['1928', 'ar', 'event', 'follow', 'footbal']...)\n",
      "Time taken: 1.6684720516204834\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dictionary = gensim.corpora.Dictionary(df['doc_processed'].tolist())\n",
    "print(dictionary)\n",
    "print(\"Time taken:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Creating BOW represention of documents </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 4), (8, 1), (9, 1), (10, 1)], [(3, 1), (5, 4), (7, 32), (9, 1), (10, 2), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 3), (20, 1), (21, 1), (22, 1), (23, 2), (24, 6), (25, 1), (26, 2), (27, 3), (28, 3), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 4), (40, 2), (41, 4), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 11), (52, 1), (53, 1), (54, 1), (55, 1), (56, 5), (57, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 4), (64, 3), (65, 3), (66, 1), (67, 4), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 12), (81, 1), (82, 1), (83, 1), (84, 3), (85, 1), (86, 1), (87, 1), (88, 4), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 4), (96, 1), (97, 1), (98, 1), (99, 4), (100, 3), (101, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (108, 2), (109, 1), (110, 1), (111, 2), (112, 4), (113, 1), (114, 1), (115, 1), (116, 3), (117, 3), (118, 1), (119, 6), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 2)], [(4, 2), (7, 4), (98, 1), (105, 2), (119, 1), (126, 2), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 2), (136, 2)], [(4, 2), (7, 7), (10, 1), (19, 3), (20, 1), (21, 1), (23, 1), (24, 2), (26, 1), (36, 1), (44, 3), (52, 2), (63, 3), (84, 1), (98, 1), (105, 4), (107, 1), (119, 1), (120, 1), (124, 3), (129, 1), (133, 1), (134, 1), (136, 2), (137, 1), (138, 3), (139, 1), (140, 2), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 3), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 2), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 2), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 2), (171, 1), (172, 1), (173, 1), (174, 1), (175, 3), (176, 1), (177, 1), (178, 1), (179, 3)], [(4, 3), (5, 2), (7, 7), (19, 3), (20, 1), (21, 1), (24, 1), (36, 2), (52, 2), (56, 2), (63, 1), (67, 1), (84, 1), (98, 2), (105, 4), (107, 1), (112, 1), (114, 1), (120, 1), (124, 1), (128, 2), (133, 1), (134, 1), (152, 1), (171, 1), (175, 1), (180, 2), (181, 1), (182, 1), (183, 3), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 3), (200, 2), (201, 1), (202, 2), (203, 1), (204, 1)]]\n",
      "Time taken: 1.5353152751922607\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['value'].tolist()]\n",
    "print(corpus[:5])\n",
    "print(\"Time taken:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Creating tf-idf model from this corpus </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017570\n",
      "Time taken: 0.39360857009887695\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)\n",
    "print(\"Time taken:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Creating Similarity index </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  15.522789716720581\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sims = gensim.similarities.Similarity('/home/rohan/Documents/Homework/WSTA',tf_idf[corpus], num_features=len(dictionary))\n",
    "print(\"Time taken: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Getting query and showing documents related to the query </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statement:  Nicholas scored the winning goal in second half\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 0.0038322623904329446), (63, 0.018533273875172247), (123, 0.20557314586856415), (200, 0.2946698961248762), (303, 0.18485970486134062), (374, 0.33995338406718256), (608, 0.41660702931168775), (1425, 0.7397623502502912)]\n",
      "<class 'numpy.ndarray'> 48228\n",
      "                                                     key  \\\n",
      "71                        1987_Football_League_Cup_Final   \n",
      "18951               1981_American_League_Division_Series   \n",
      "14385                     1982_Scottish_League_Cup_Final   \n",
      "44488                      1976â€“77_Iraqi_National_League   \n",
      "8426   1914_All-Ireland_Senior_Football_Championship_...   \n",
      "\n",
      "                                                   value  similarity_score  \n",
      "71     [the, 1987, footbal, leagu, cup, final, wa, a,...          0.348573  \n",
      "18951  [the, 1981, american, leagu, divis, seri, ald,...          0.276400  \n",
      "14385  [the, 1982, scottish, leagu, cup, final, wa, p...          0.271995  \n",
      "44488  [the, 1976, 77, iraqi, nation, leagu, wa, the,...          0.240028  \n",
      "8426   [the, 1914, all, ireland, senior, footbal, cha...          0.207636  \n",
      "Time taken: 1.0913012027740479\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter statement: \")\n",
    "start = time.time()\n",
    "query_doc = preprocess(query)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)\n",
    "sim_list = sims[query_doc_tf_idf]\n",
    "df['similarity_score'] = sim_list\n",
    "print(type(sim_list), len(sim_list))\n",
    "print(df.nlargest(5, ['similarity_score']))\n",
    "#print(\"Position of most relevant doc: \", max_index)\n",
    "print(\"Time taken:\", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
